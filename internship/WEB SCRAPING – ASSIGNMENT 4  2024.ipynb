{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79ea0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "driver_path = 'path_to_your_chromedriver' \n",
    "driver = webdriver.Chrome(driver_path)\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos'\n",
    "\n",
    "try:\n",
    "    \n",
    "    driver.get(url)\n",
    "    \n",
    "    \n",
    "    table = driver.find_element(By.CLASS_NAME, 'wikitable')\n",
    "\n",
    "    \n",
    "    rows = table.find_elements(By.TAG_NAME, 'tr')\n",
    "\n",
    "    \n",
    "    data = []\n",
    "\n",
    "    for row in rows[1:]:  \n",
    "        cols = row.find_elements(By.TAG_NAME, 'td')\n",
    "        if len(cols) > 0:\n",
    "            rank = cols[0].text.strip()\n",
    "            name = cols[1].text.strip()\n",
    "            artist = cols[2].text.strip()\n",
    "            upload_date = cols[3].text.strip()\n",
    "            views = cols[4].text.strip()\n",
    "            data.append([rank, name, artist, upload_date, views])\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame(data, columns=['Rank', 'Name', 'Artist', 'Upload date', 'Views'])\n",
    "    \n",
    "    \n",
    "    print(df)\n",
    "    df.to_csv('most_viewed_youtube_videos.csv', index=False)\n",
    "\n",
    "except NoSuchElementException as e:\n",
    "    print(f'Error: {e}')\n",
    "except TimeoutException as e:\n",
    "    print(f'Timeout Error: {e}')\n",
    "finally:\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8c4656f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\komal\\anaconda3\\lib\\site-packages (4.21.0)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\komal\\anaconda3\\lib\\site-packages (from selenium) (1.26.16)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\komal\\anaconda3\\lib\\site-packages (from selenium) (0.25.1)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\komal\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\komal\\anaconda3\\lib\\site-packages (from selenium) (2023.7.22)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\komal\\anaconda3\\lib\\site-packages (from selenium) (4.12.1)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\komal\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\komal\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\komal\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\komal\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\komal\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\komal\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\komal\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\komal\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\komal\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\komal\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c08158e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, WebDriverException, TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "try:\n",
    "    \n",
    "    driver.get('https://www.bcci.tv/')\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    \n",
    "    fixtures_link = wait.until(EC.element_to_be_clickable((By.LINK_TEXT, 'FIXTURES')))\n",
    "    fixtures_link.click()\n",
    "\n",
    "    \n",
    "    wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'fixture-card')))\n",
    "\n",
    "    \n",
    "    fixtures = driver.find_elements(By.XPATH, '//div[@class=\"fixture-card\"]')\n",
    "\n",
    "    \n",
    "    for fixture in fixtures:\n",
    "        try:\n",
    "            series = fixture.find_element(By.CLASS_NAME, 'fixture-card__series').text\n",
    "            place = fixture.find_element(By.CLASS_NAME, 'fixture-card__venue').text\n",
    "            datetime = fixture.find_element(By.CLASS_NAME, 'fixture-card__datetime').text.split(' | ')\n",
    "            date = datetime[0].strip()\n",
    "            time = datetime[1].strip() if len(datetime) > 1 else 'TBD'\n",
    "            \n",
    "            print(f'Series: {series}, Place: {place}, Date: {date}, Time: {time}')\n",
    "        except NoSuchElementException as e:\n",
    "            print(\"Element not found in fixture:\", e)\n",
    "\n",
    "except (NoSuchElementException, WebDriverException, TimeoutException) as e:\n",
    "    print(\"Error occurred:\", e)\n",
    "finally:\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b5f5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException, WebDriverException\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "try:\n",
    "    \n",
    "    driver.get('http://statisticstimes.com/')\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    \n",
    "    economy_link = wait.until(EC.element_to_be_clickable((By.LINK_TEXT, 'Economy')))\n",
    "    economy_link.click()\n",
    "\n",
    "    \n",
    "    india_link = wait.until(EC.element_to_be_clickable((By.LINK_TEXT, 'India')))\n",
    "    india_link.click()\n",
    "\n",
    "    \n",
    "    gdp_link = wait.until(EC.element_to_be_clickable((By.LINK_TEXT, 'GDP of Indian states')))\n",
    "    gdp_link.click()\n",
    "\n",
    "    \n",
    "    table = wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"table_id\"]/tbody')))\n",
    "\n",
    "    \n",
    "    rows = table.find_elements(By.TAG_NAME, 'tr')\n",
    "\n",
    "    \n",
    "    for row in rows:\n",
    "        cells = row.find_elements(By.TAG_NAME, 'td')\n",
    "        if len(cells) > 5:\n",
    "            rank = cells[0].text\n",
    "            state = cells[1].text\n",
    "            gsdp_18_19 = cells[2].text\n",
    "            gsdp_19_20 = cells[3].text\n",
    "            share_18_19 = cells[4].text\n",
    "            gdp = cells[5].text\n",
    "\n",
    "            print(f'Rank: {rank}, State: {state}, GSDP(18-19): {gsdp_18_19}, GSDP(19-20): {gsdp_19_20}, Share(18-19): {share_18_19}, GDP($ billion): {gdp}')\n",
    "\n",
    "except (NoSuchElementException, TimeoutException, WebDriverException) as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "finally:\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eda795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException, WebDriverException\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "try:\n",
    "    \n",
    "    driver.get('https://github.com/')\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    \n",
    "    explore_menu = wait.until(EC.element_to_be_clickable((By.XPATH, '//summary[contains(text(),\"Explore\")]')))\n",
    "    explore_menu.click()\n",
    "\n",
    "    \n",
    "    trending_option = wait.until(EC.element_to_be_clickable((By.XPATH, '//a[contains(text(),\"Trending\")]')))\n",
    "    trending_option.click()\n",
    "\n",
    "    \n",
    "    wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'Box-row')))\n",
    "\n",
    "    \n",
    "    repositories = driver.find_elements(By.CLASS_NAME, 'Box-row')\n",
    "\n",
    "    \n",
    "    for repo in repositories:\n",
    "        try:\n",
    "            title = repo.find_element(By.CSS_SELECTOR, 'h1.h3 a').text\n",
    "            description = repo.find_element(By.CSS_SELECTOR, 'p.col-9').text if repo.find_elements(By.CSS_SELECTOR, 'p.col-9') else 'No description'\n",
    "            contributors = repo.find_element(By.XPATH, './/span[@class=\"d-inline-block mr-3\"]//a').text if repo.find_elements(By.XPATH, './/span[@class=\"d-inline-block mr-3\"]//a') else 'No contributors'\n",
    "            language = repo.find_element(By.XPATH, './/span[@itemprop=\"programmingLanguage\"]').text if repo.find_elements(By.XPATH, './/span[@itemprop=\"programmingLanguage\"]') else 'No language specified'\n",
    "            \n",
    "            print(f'Repository title: {title}, Description: {description}, Contributors count: {contributors}, Language: {language}')\n",
    "        except NoSuchElementException as e:\n",
    "            print(\"Element not found in repository:\", e)\n",
    "\n",
    "except (NoSuchElementException, TimeoutException, WebDriverException) as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "finally:\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82949b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.billboard.com/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "songs = soup.find_all('div', class_='chart-row__title')\n",
    "artists = soup.find_all('div', class_='chart-row__artist')\n",
    "last_week_ranks = soup.find_all('div', class_='chart-row__last-week')\n",
    "peak_ranks = soup.find_all('div', class_='chart-row__top-spot')\n",
    "weeks_on_board = soup.find_all('div', class_='chart-row__weeks-on-chart')\n",
    "\n",
    "\n",
    "for i in range(len(songs)):\n",
    "    song_name = songs[i].text.strip()\n",
    "    artist_name = artists[i].text.strip()\n",
    "    last_week_rank = last_week_ranks[i].text.strip()\n",
    "    peak_rank = peak_ranks[i].text.strip()\n",
    "    weeks_on_board = weeks_on_board[i].text.strip()\n",
    "    \n",
    "    print(f\"Song: {song_name}\")\n",
    "    print(f\"Artist: {artist_name}\")\n",
    "    print(f\"Last Week Rank: {last_week_rank}\")\n",
    "    print(f\"Peak Rank: {peak_rank}\")\n",
    "    print(f\"Weeks on Board: {weeks_on_board}\")\n",
    "    print(\"-----------------------------------\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c36d548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "book_entries = soup.find_all('table', class_='interactive')\n",
    "\n",
    "for entry in book_entries:\n",
    "    \n",
    "    rows = entry.find_all('tr')\n",
    "    for row in rows:\n",
    "        columns = row.find_all('td')\n",
    "        if len(columns) >= 5:  \n",
    "            book_name = columns[0].text.strip()\n",
    "            author_name = columns[1].text.strip()\n",
    "            volumes_sold = columns[2].text.strip()\n",
    "            publisher = columns[3].text.strip()\n",
    "            genre = columns[4].text.strip()\n",
    "            \n",
    "            \n",
    "            print(f\"Book: {book_name}\")\n",
    "            print(f\"Author: {author_name}\")\n",
    "            print(f\"Volumes Sold: {volumes_sold}\")\n",
    "            print(f\"Publisher: {publisher}\")\n",
    "            print(f\"Genre: {genre}\")\n",
    "            print(\"-----------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c41cdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.imdb.com/list/ls095964455/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "series_entries = soup.find_all('div', class_='lister-item-content')\n",
    "\n",
    "for entry in series_entries:\n",
    "    \n",
    "    name = entry.find('h3', class_='lister-item-header').a.text.strip()\n",
    "    year_span = entry.find('span', class_='lister-item-year').text.strip()\n",
    "    genre = entry.find('span', class_='genre').text.strip()\n",
    "    run_time = entry.find('span', class_='runtime').text.strip()\n",
    "    ratings = entry.find('span', class_='ipl-rating-star__rating').text.strip()\n",
    "    votes = entry.find('span', attrs={'name': 'nv'}).text.strip()\n",
    "    \n",
    "    \n",
    "    print(f\"Name: {name}\")\n",
    "    print(f\"Year Span: {year_span}\")\n",
    "    print(f\"Genre: {genre}\")\n",
    "    print(f\"Run Time: {run_time}\")\n",
    "    print(f\"Ratings: {ratings}\")\n",
    "    print(f\"Votes: {votes}\")\n",
    "    print(\"-----------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d431bff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "\n",
    "base_url = \"https://archive.ics.uci.edu/\"\n",
    "main_url = base_url + \"ml/index.php\"\n",
    "response = requests.get(main_url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "datasets_page_link = None\n",
    "for link in soup.find_all('a', href=True):\n",
    "    if 'datasets' in link['href'].lower():\n",
    "        datasets_page_link = base_url + link['href']\n",
    "        break\n",
    "\n",
    "if datasets_page_link:\n",
    "   \n",
    "    datasets_response = requests.get(datasets_page_link)\n",
    "    datasets_soup = BeautifulSoup(datasets_response.content, 'html.parser')\n",
    "\n",
    "    \n",
    "    dataset_entries = datasets_soup.find_all('p', class_='normal')\n",
    "\n",
    "    for entry in dataset_entries:\n",
    "        \n",
    "        dataset_name = entry.find('a').text.strip()\n",
    "        dataset_details_url = base_url + entry.find('a')['href']\n",
    "        dataset_details_response = requests.get(dataset_details_url)\n",
    "        dataset_details_soup = BeautifulSoup(dataset_details_response.content, 'html.parser')\n",
    "        \n",
    "        \n",
    "        details_text = dataset_details_soup.find('pre').text\n",
    "        \n",
    "        \n",
    "        data_type = re.search(r\"Data Type: (.+)\", details_text).group(1).strip()\n",
    "        task = re.search(r\"Default Task: (.+)\", details_text).group(1).strip()\n",
    "        attribute_type = re.search(r\"Attribute Type: (.+)\", details_text).group(1).strip()\n",
    "        num_instances = re.search(r\"# Instances: (\\d+)\", details_text).group(1).strip()\n",
    "        num_attributes = re.search(r\"# Attributes: (\\d+)\", details_text).group(1).strip()\n",
    "        year = re.search(r\"Year: (\\d+)\", details_text).group(1).strip()\n",
    "\n",
    "        \n",
    "        print(f\"Dataset Name: {dataset_name}\")\n",
    "        print(f\"Data Type: {data_type}\")\n",
    "        print(f\"Task: {task}\")\n",
    "        print(f\"Attribute Type: {attribute_type}\")\n",
    "        print(f\"Number of Instances: {num_instances}\")\n",
    "        print(f\"Number of Attributes: {num_attributes}\")\n",
    "        print(f\"Year: {year}\")\n",
    "        print(\"-----------------------------------\")\n",
    "else:\n",
    "    print(\"Couldn't find the link to all datasets page.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a0f234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91fba0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad8e679",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
