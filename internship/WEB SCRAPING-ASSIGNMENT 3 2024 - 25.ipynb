{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd72fb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\komal\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\komal\\anaconda3\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\komal\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\komal\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\komal\\anaconda3\\lib\\site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\komal\\anaconda3\\lib\\site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\komal\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5263b4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_amazon(product):\n",
    "    \n",
    "    product = product.replace(' ', '+')\n",
    "\n",
    "    \n",
    "    url = f\"https://www.amazon.in/s?k={product}\"\n",
    "\n",
    "    \n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "    products = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
    "\n",
    "    \n",
    "    for product in products:\n",
    "        title = product.h2.text\n",
    "        link = 'https://www.amazon.in' + product.h2.a['href']\n",
    "        try:\n",
    "            price = product.find('span', 'a-price').find('span', 'a-offscreen').text\n",
    "        except AttributeError:\n",
    "            price = \"Price not available\"\n",
    "        print(f\"Product: {title}, Price: {price}, Link: {link}\")\n",
    "\n",
    "\n",
    "product_to_search = input(\"Enter the product you want to search for: \")\n",
    "search_amazon(product_to_search)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063b7c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "def scrape_product_details(search_query):\n",
    "    \n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get('https://www.example.com/')  \n",
    "    \n",
    "    \n",
    "    search_box = driver.find_element_by_name('search')\n",
    "    search_box.send_keys(search_query)\n",
    "    search_box.send_keys(Keys.ENTER)\n",
    "    \n",
    "    \n",
    "    time.sleep(5)\n",
    "    \n",
    "    \n",
    "    products_list = []\n",
    "    \n",
    "    \n",
    "    for page in range(1, 4):\n",
    "        \n",
    "        products = driver.find_elements_by_css_selector('div.product')  \n",
    "        \n",
    "        for product in products:\n",
    "            \n",
    "            brand_name = product.find_element_by_css_selector('...').text or \"-\"\n",
    "            product_name = product.find_element_by_css_selector('...').text or \"-\"\n",
    "            price = product.find_element_by_css_selector('...').text or \"-\"\n",
    "            return_exchange = product.find_element_by_css_selector('...').text or \"-\"\n",
    "            expected_delivery = product.find_element_by_css_selector('...').text or \"-\"\n",
    "            availability = product.find_element_by_css_selector('...').text or \"-\"\n",
    "            product_url = product.find_element_by_css_selector('a').get_attribute('href') or \"-\"\n",
    "            \n",
    "            \n",
    "            products_list.append({\n",
    "                \"Brand Name\": brand_name,\n",
    "                \"Name of the Product\": product_name,\n",
    "                \"Price\": price,\n",
    "                \"Return/Exchange\": return_exchange,\n",
    "                \"Expected Delivery\": expected_delivery,\n",
    "                \"Availability\": availability,\n",
    "                \"Product URL\": product_url\n",
    "            })\n",
    "        \n",
    "        \n",
    "        next_page_button = driver.find_element_by_css_selector('...')  \n",
    "        if next_page_button:\n",
    "            next_page_button.click()\n",
    "            time.sleep(5)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame(products_list)\n",
    "    df.to_csv(f'{search_query}_products.csv', index=False)\n",
    "    \n",
    "scrape_product_details('example search query') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8efd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "\n",
    "\n",
    "def fetch_images(search_query, number_of_images):\n",
    "    \n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get('https://images.google.com/')\n",
    "    \n",
    "    \n",
    "    search_box = driver.find_element_by_name('q')\n",
    "    search_box.send_keys(search_query)\n",
    "    search_box.send_keys(Keys.ENTER)\n",
    "    \n",
    "    \n",
    "    time.sleep(5)\n",
    "    \n",
    "    \n",
    "    images = driver.find_elements_by_css_selector('img.rg_i.Q4LuWd')\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(search_query):\n",
    "        os.makedirs(search_query)\n",
    "    \n",
    "    \n",
    "    for i in range(number_of_images):\n",
    "        try:\n",
    "            \n",
    "            images[i].click()\n",
    "            time.sleep(2)\n",
    "            \n",
    "            \n",
    "            large_image = driver.find_elements_by_css_selector('img.n3VNCb')[1]\n",
    "            image_url = large_image.get_attribute('src')\n",
    "            \n",
    "            \n",
    "            response = requests.get(image_url)\n",
    "            with open(f'{search_query}/{search_query}_{i+1}.jpg', 'wb') as file:\n",
    "                file.write(response.content)\n",
    "        except Exception as e:\n",
    "            print(f\"Error - {e}\")\n",
    "    \n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "for keyword in keywords:\n",
    "    fetch_images(keyword, 10) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c06b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_flipkart(search_query):\n",
    "    \n",
    "    url = f\"https://www.flipkart.com/search?q={search_query}\"\n",
    "    \n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    \n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    \n",
    "    product_containers = soup.find_all(\"div\", class_=\"_1AtVbE\")\n",
    "\n",
    "    \n",
    "    products = []\n",
    "\n",
    "    for container in product_containers:\n",
    "        try:\n",
    "           \n",
    "            name_info = container.find(\"div\", class_=\"_4rR01T\")\n",
    "            if name_info:\n",
    "                name = name_info.text.split(\" \", 1)\n",
    "                brand_name = name[0]\n",
    "                smartphone_name = name[1]\n",
    "            else:\n",
    "                brand_name = \"-\"\n",
    "                smartphone_name = \"-\"\n",
    "            \n",
    "            # Product URL\n",
    "            product_url_tag = container.find(\"a\", class_=\"_1fQZEK\")\n",
    "            product_url = \"https://www.flipkart.com\" + product_url_tag['href'] if product_url_tag else \"-\"\n",
    "            \n",
    "            # Price\n",
    "            price_tag = container.find(\"div\", class_=\"_30jeq3 _1_WHN1\")\n",
    "            price = price_tag.text if price_tag else \"-\"\n",
    "            \n",
    "            # Features\n",
    "            features = container.find_all(\"li\", class_=\"rgWa7D\")\n",
    "            feature_dict = {\n",
    "                \"Colour\": \"-\",\n",
    "                \"RAM\": \"-\",\n",
    "                \"Storage(ROM)\": \"-\",\n",
    "                \"Primary Camera\": \"-\",\n",
    "                \"Secondary Camera\": \"-\",\n",
    "                \"Display Size\": \"-\",\n",
    "                \"Battery Capacity\": \"-\"\n",
    "            }\n",
    "            for feature in features:\n",
    "                feature_text = feature.text\n",
    "                if \"RAM\" in feature_text and \"ROM\" in feature_text:\n",
    "                    ram, rom = feature_text.split(\"|\")\n",
    "                    feature_dict[\"RAM\"] = ram.strip()\n",
    "                    feature_dict[\"Storage(ROM)\"] = rom.strip()\n",
    "                elif \"Display\" in feature_text:\n",
    "                    feature_dict[\"Display Size\"] = feature_text\n",
    "                elif \"Camera\" in feature_text and \"Primary\" not in feature_dict[\"Primary Camera\"]:\n",
    "                    feature_dict[\"Primary Camera\"] = feature_text\n",
    "                elif \"Battery\" in feature_text:\n",
    "                    feature_dict[\"Battery Capacity\"] = feature_text\n",
    "                else:\n",
    "                    # Check if the feature is one of the secondary camera features\n",
    "                    if \"Secondary Camera\" not in feature_dict[\"Secondary Camera\"]:\n",
    "                        feature_dict[\"Secondary Camera\"] = feature_text\n",
    "\n",
    "            \n",
    "            products.append({\n",
    "                \"Brand Name\": brand_name,\n",
    "                \"Smartphone name\": smartphone_name,\n",
    "                \"Colour\": feature_dict[\"Colour\"],\n",
    "                \"RAM\": feature_dict[\"RAM\"],\n",
    "                \"Storage(ROM)\": feature_dict[\"Storage(ROM)\"],\n",
    "                \"Primary Camera\": feature_dict[\"Primary Camera\"],\n",
    "                \"Secondary Camera\": feature_dict[\"Secondary Camera\"],\n",
    "                \"Display Size\": feature_dict[\"Display Size\"],\n",
    "                \"Battery Capacity\": feature_dict[\"Battery Capacity\"],\n",
    "                \"Price\": price,\n",
    "                \"Product URL\": product_url\n",
    "            })\n",
    "        except Exception as e:\n",
    "    \n",
    "            print(f\"Error processing product: {e}\")\n",
    "            continue\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame(products)\n",
    "    \n",
    "    \n",
    "    df.to_csv(\"flipkart_smartphones.csv\", index=False)\n",
    "    print(\"Data saved to flipkart_smartphones.csv\")\n",
    "\n",
    "\n",
    "scrape_flipkart(\"Oneplus Nord\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b348bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "def get_geospatial_coordinates(city_name):\n",
    "    \n",
    "    geolocator = Nominatim(user_agent=\"geoapiExercises\")\n",
    "    \n",
    "\n",
    "    location = geolocator.geocode(city_name)\n",
    "    \n",
    "    if location:\n",
    "        return location.latitude, location.longitude\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "\n",
    "city_name = \"San Francisco\"\n",
    "latitude, longitude = get_geospatial_coordinates(city_name)\n",
    "\n",
    "if latitude and longitude:\n",
    "    print(f\"The latitude and longitude of {city_name} are {latitude}, {longitude}.\")\n",
    "else:\n",
    "    print(f\"Coordinates for {city_name} could not be found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637deae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_digit_gaming_laptops():\n",
    "    \n",
    "    url = \"https://www.digit.in/top-products/best-gaming-laptops-40.html\"\n",
    "    \n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    \n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    \n",
    "    laptop_containers = soup.find_all(\"div\", class_=\"TopNumbeHeading sticky-footer\")\n",
    "\n",
    "    \n",
    "    laptops = []\n",
    "\n",
    "    for container in laptop_containers:\n",
    "        try:\n",
    "            \n",
    "            name_tag = container.find(\"div\", class_=\"left_side\")\n",
    "            name = name_tag.h3.text if name_tag else \"-\"\n",
    "            \n",
    "            \n",
    "            details = container.find_all(\"div\", class_=\"Specs-list\")\n",
    "            detail_dict = {\n",
    "                \"Laptop Name\": name,\n",
    "                \"Processor\": \"-\",\n",
    "                \"Graphics Processor\": \"-\",\n",
    "                \"RAM\": \"-\",\n",
    "                \"Screen Size\": \"-\",\n",
    "                \"Resolution\": \"-\",\n",
    "                \"Weight\": \"-\",\n",
    "                \"Price\": \"-\"\n",
    "            }\n",
    "            for detail in details:\n",
    "                detail_text = detail.text.split(':')\n",
    "                if len(detail_text) == 2:\n",
    "                    key, value = detail_text\n",
    "                    detail_dict[key.strip()] = value.strip()\n",
    "            \n",
    "            \n",
    "            laptops.append(detail_dict)\n",
    "        except Exception as e:\n",
    "            \n",
    "            print(f\"Error processing laptop: {e}\")\n",
    "            continue\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame(laptops)\n",
    "    \n",
    "\n",
    "    df.to_csv(\"digit_gaming_laptops.csv\", index=False)\n",
    "    print(\"Data saved to digit_gaming_laptops.csv\")\n",
    "\n",
    "\n",
    "scrape_digit_gaming_laptops()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd16cad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_forbes_billionaires():\n",
    "    \n",
    "    url = \"https://www.forbes.com/billionaires/\"\n",
    "    \n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    \n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    \n",
    "    script = soup.find(\"script\", {\"type\": \"application/json\"})\n",
    "    json_data = script.string\n",
    "    \n",
    "    \n",
    "    import json\n",
    "    data = json.loads(json_data)\n",
    "    \n",
    "\n",
    "    billionaires = data[\"props\"][\"pageProps\"][\"tableData\"]\n",
    "\n",
    "    \n",
    "    details = []\n",
    "\n",
    "    for billionaire in billionaires:\n",
    "        details.append({\n",
    "            \"Rank\": billionaire.get(\"rank\", \"-\"),\n",
    "            \"Name\": billionaire.get(\"personName\", \"-\"),\n",
    "            \"Net worth\": billionaire.get(\"finalWorth\", \"-\"),\n",
    "            \"Age\": billionaire.get(\"age\", \"-\"),\n",
    "            \"Citizenship\": billionaire.get(\"countryOfCitizenship\", \"-\"),\n",
    "            \"Source\": billionaire.get(\"source\", \"-\"),\n",
    "            \"Industry\": billionaire.get(\"industries\", \"-\")\n",
    "        })\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame(details)\n",
    "    \n",
    "\n",
    "    df.to_csv(\"forbes_billionaires.csv\", index=False)\n",
    "    print(\"Data saved to forbes_billionaires.csv\")\n",
    "\n",
    "\n",
    "scrape_forbes_billionaires() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b35ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import googleapiclient.discovery\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_youtube_comments(video_id, api_key, max_comments=500):\n",
    "\n",
    "    youtube = googleapiclient.discovery.build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    \n",
    "    request = youtube.commentThreads().list(\n",
    "        part=\"snippet\",\n",
    "        videoId=video_id,\n",
    "        maxResults=100, \n",
    "        textFormat=\"plainText\"\n",
    "    )\n",
    "    \n",
    "    comments = []\n",
    "    comment_count = 0\n",
    "\n",
    "    while request and comment_count < max_comments:\n",
    "        response = request.execute()\n",
    "\n",
    "        for item in response['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']\n",
    "            comments.append({\n",
    "                'Comment': comment['textDisplay'],\n",
    "                'Upvotes': comment['likeCount'],\n",
    "                'Published At': comment['publishedAt']\n",
    "            })\n",
    "            comment_count += 1\n",
    "\n",
    "        \n",
    "        if comment_count >= max_comments:\n",
    "            break\n",
    "\n",
    "        \n",
    "        request = youtube.commentThreads().list_next(request, response)\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame(comments)\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    API_KEY = \"YOUR_YOUTUBE_API_KEY\"\n",
    "    VIDEO_ID = \"YOUR_VIDEO_ID\"  \n",
    "    MAX_COMMENTS = 500\n",
    "\n",
    "    comments_df = get_youtube_comments(VIDEO_ID, API_KEY, MAX_COMMENTS)\n",
    "    comments_df.to_csv(\"youtube_comments.csv\", index=False)\n",
    "    print(\"Comments saved to youtube_comments.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50efa167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_hostelworld_london():\n",
    "    url = \"https://www.hostelworld.com/findabed.php/ChosenCity.London/ChosenCountry.England\"\n",
    "    \n",
    "    # Send the request to hostelworld.com\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Find all the hostel containers on the page\n",
    "    hostels = soup.find_all(\"div\", class_=\"property-card\")\n",
    "    \n",
    "    # Prepare a list to hold all the hostel details\n",
    "    hostel_details = []\n",
    "\n",
    "    for hostel in hostels:\n",
    "        try:\n",
    "            \n",
    "            name = hostel.find(\"h2\", class_=\"title\").text.strip()\n",
    "\n",
    "        \n",
    "            distance = hostel.find(\"span\", class_=\"distance-description\").text.strip()\n",
    "\n",
    "            \n",
    "            rating_tag = hostel.find(\"div\", class_=\"score orange big\")\n",
    "            rating = rating_tag.text.strip() if rating_tag else \"-\"\n",
    "\n",
    "            \n",
    "            total_reviews_tag = hostel.find(\"div\", class_=\"reviews\")\n",
    "            total_reviews = total_reviews_tag.text.strip() if total_reviews_tag else \"-\"\n",
    "\n",
    "    \n",
    "            overall_reviews_tag = hostel.find(\"div\", class_=\"keyword\")\n",
    "            overall_reviews = overall_reviews_tag.text.strip() if overall_reviews_tag else \"-\"\n",
    "\n",
    "            \n",
    "            privates_price_tag = hostel.find(\"div\", class_=\"price title-2\")\n",
    "            privates_from_price = privates_price_tag.text.strip() if privates_price_tag else \"-\"\n",
    "\n",
    "            dorms_price_tag = hostel.find(\"div\", class_=\"price dorm\")\n",
    "            dorms_from_price = dorms_price_tag.text.strip() if dorms_price_tag else \"-\"\n",
    "\n",
    "            \n",
    "            facilities_list = hostel.find_all(\"ul\", class_=\"facility-segment\")\n",
    "            facilities = [facility.text.strip() for facility in facilities_list]\n",
    "\n",
    "            \n",
    "            description_tag = hostel.find(\"div\", class_=\"rating-factors\")\n",
    "            property_description = description_tag.text.strip() if description_tag else \"-\"\n",
    "\n",
    "            \n",
    "            hostel_details.append({\n",
    "                \"Hostel Name\": name,\n",
    "                \"Distance from City Centre\": distance,\n",
    "                \"Ratings\": rating,\n",
    "                \"Total Reviews\": total_reviews,\n",
    "                \"Overall Reviews\": overall_reviews,\n",
    "                \"Privates From Price\": privates_from_price,\n",
    "                \"Dorms From Price\": dorms_from_price,\n",
    "                \"Facilities\": \", \".join(facilities),\n",
    "                \"Property Description\": property_description\n",
    "            })\n",
    "        except Exception as e:\n",
    "            \n",
    "            print(f\"Error processing hostel: {e}\")\n",
    "            continue\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame(hostel_details)\n",
    "    \n",
    "    \n",
    "    df.to_csv(\"hostelworld_london.csv\", index=False)\n",
    "    print(\"Data saved to hostelworld_london.csv\")\n",
    "\n",
    "    \n",
    "scrape_hostelworld_london()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d2e9be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a915d0f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdc81a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fed2885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afc7099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11513e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c62f94d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ae8e05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5191f789",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
