{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "698bc562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\komal\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\komal\\anaconda3\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\komal\\anaconda3\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\komal\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\komal\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\komal\\anaconda3\\lib\\site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\komal\\anaconda3\\lib\\site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\komal\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\komal\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\komal\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\komal\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\komal\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\komal\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bf561be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Name, Rating, Release Year]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the IMDb list\n",
    "url = \"https://www.imdb.com/list/ls056092300/\"\n",
    "\n",
    "# Send an HTTP request to fetch the webpage content\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the movie entries\n",
    "movie_entries = soup.find_all(\"div\", class_=\"lister-item-content\")\n",
    "\n",
    "# Initialize lists to store movie data\n",
    "names = []\n",
    "ratings = []\n",
    "release_years = []\n",
    "\n",
    "# Extract data for each movie\n",
    "for entry in movie_entries:\n",
    "    # Extract movie name\n",
    "    name = entry.h3.a.text\n",
    "    names.append(name)\n",
    "\n",
    "    # Extract movie rating\n",
    "    rating = float(entry.find(\"span\", class_=\"ipl-rating-star__rating\").text)\n",
    "    ratings.append(rating)\n",
    "\n",
    "    # Extract movie release year\n",
    "    year = int(entry.find(\"span\", class_=\"lister-item-year\").text.strip(\"()\"))\n",
    "    release_years.append(year)\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"Name\": names,\n",
    "    \"Rating\": ratings,\n",
    "    \"Release Year\": release_years\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f17f2232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# The URL of the Patreon page to scrape\n",
    "url = 'https://www.patreon.com/coreyms'\n",
    "\n",
    "# Send a GET request to the page\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all post entries\n",
    "posts = soup.find_all('div', class_='post')\n",
    "\n",
    "# Initialize a list to store scraped data\n",
    "scraped_data = []\n",
    "\n",
    "# Loop through each post and extract details\n",
    "for post in posts:\n",
    "    # Extract the heading\n",
    "    heading = post.find('a', class_='post-title').text.strip()\n",
    "\n",
    "    # Extract the date\n",
    "    date = post.find('time', class_='post-date').text.strip()\n",
    "\n",
    "    # Extract the content\n",
    "    content = post.find('div', class_='post-content').text.strip()\n",
    "\n",
    "    # Extract the likes (if available)\n",
    "    likes = post.find('span', class_='likes-count')\n",
    "    likes = likes.text.strip() if likes else 'N/A'\n",
    "\n",
    "    # Extract the YouTube video link (if available)\n",
    "    video_link = post.find('a', class_='youtube-link')\n",
    "    video_link = video_link['href'] if video_link else 'N/A'\n",
    "\n",
    "    # Append the extracted details to the list\n",
    "    scraped_data.append({\n",
    "        'Heading': heading,\n",
    "        'Date': date,\n",
    "        'Content': content,\n",
    "        'Likes': likes,\n",
    "        'YouTube Link': video_link\n",
    "    })\n",
    "\n",
    "# Convert the list to JSON format\n",
    "json_data = json.dumps(scraped_data, indent=4)\n",
    "\n",
    "# Print the JSON data\n",
    "print(json_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68c79634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Base URL of NoBroker\n",
    "base_url = 'https://www.nobroker.in/'\n",
    "\n",
    "# Localities to search for\n",
    "localities = ['Indira+Nagar', 'Jayanagar', 'Rajaji+Nagar']\n",
    "\n",
    "# List to store scraped data\n",
    "houses = []\n",
    "\n",
    "# Headers to mimic a browser visit\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "for locality in localities:\n",
    "    # Update URL with the current locality\n",
    "    url = f'{base_url}property-for-sale-in-{locality}-bangalore'\n",
    "\n",
    "    # Send GET request\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all listings in the locality\n",
    "    listings = soup.find_all('div', class_='nb__2JHKO')\n",
    "\n",
    "    # Extract details from each listing\n",
    "    for listing in listings:\n",
    "        # Extract house title\n",
    "        title = listing.find('span', class_='nb__2tSz-').text\n",
    "\n",
    "        # Extract location\n",
    "        location = listing.find('div', class_='nb__35Ol7').text\n",
    "\n",
    "        # Extract area\n",
    "        area = listing.find('div', class_='nb__3oNyC').text\n",
    "\n",
    "        # Extract EMI, if available\n",
    "        emi = listing.find('div', class_='font-semi-bold heading-6', id='roomType')\n",
    "        emi = emi.text.split('₹')[1] if emi else 'N/A'\n",
    "\n",
    "        # Extract price\n",
    "        price = listing.find('div', class_='font-semi-bold heading-6').text\n",
    "\n",
    "        # Append to the list\n",
    "        houses.append({\n",
    "            'Title': title,\n",
    "            'Location': location,\n",
    "            'Area': area,\n",
    "            'EMI': emi,\n",
    "            'Price': price\n",
    "        })\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(houses)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ca0399f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Product Name     Price  \\\n",
      "0        bewakoof x disney  No price   \n",
      "1  bewakoof x looney tunes  No price   \n",
      "2      bewakoof x garfield  No price   \n",
      "3                Bewakoof®  No price   \n",
      "4                Bewakoof®  No price   \n",
      "5        Bewakoof Air® 1.0  No price   \n",
      "6                Bewakoof®  No price   \n",
      "7        Bewakoof Air® 1.0  No price   \n",
      "8                Bewakoof®  No price   \n",
      "9        Bewakoof Air® 1.0  No price   \n",
      "\n",
      "                                           Image URL  \n",
      "0  https://images.bewakoof.com/t640/women-s-white...  \n",
      "1  https://images.bewakoof.com/t640/women-aop-ove...  \n",
      "2  https://images.bewakoof.com/t640/women-aop-ove...  \n",
      "3  https://images.bewakoof.com/t640/women-white-p...  \n",
      "4  https://images.bewakoof.com/t640/women-s-pink-...  \n",
      "5  https://images.bewakoof.com/t640/men-s-black-o...  \n",
      "6  https://images.bewakoof.com/t640/men-s-sky-blu...  \n",
      "7  https://images.bewakoof.com/t640/men-s-purple-...  \n",
      "8  https://images.bewakoof.com/t640/women-s-blue-...  \n",
      "9  https://images.bewakoof.com/t640/men-s-blue-ov...  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the Bewakoof bestseller page\n",
    "url = \"https://www.bewakoof.com/bestseller?sort=popular\"\n",
    "\n",
    "# Fetch the content of the page\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Initialize lists to hold the data\n",
    "product_names = []\n",
    "prices = []\n",
    "image_urls = []\n",
    "\n",
    "# Find the container that holds all product items\n",
    "product_containers = soup.find_all('div', class_='productCardBox')[:10]\n",
    "\n",
    "# Loop through each container and extract the required information\n",
    "for container in product_containers:\n",
    "    # Product Name\n",
    "    name = container.find('h3').text.strip() if container.find('h3') else 'No name'\n",
    "    product_names.append(name)\n",
    "    \n",
    "    # Price\n",
    "    price = container.find('span', class_='discountedPriceText').text.strip() if container.find('span', class_='discountedPriceText') else 'No price'\n",
    "    prices.append(price)\n",
    "    \n",
    "    # Image URL\n",
    "    image_url = container.find('img')['src'] if container.find('img') else 'No image URL'\n",
    "    image_urls.append(image_url)\n",
    "\n",
    "# Create a DataFrame\n",
    "products_df = pd.DataFrame({\n",
    "    'Product Name': product_names,\n",
    "    'Price': prices,\n",
    "    'Image URL': image_urls\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(products_df)\n",
    "\n",
    "# Optionally, save the DataFrame to a CSV file\n",
    "products_df.to_csv('bewakoof_bestsellers.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce4c3619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heading: Treasury yields hold steady as investors look to data, Fed speaker comments in week ahead\n",
      "Date: No date provided\n",
      "Link: https://www.cnbc.com/2024/05/20/us-treasurys-investors-look-to-data-fed-comments-in-week-ahead.html\n",
      "--------------------------------------------------\n",
      "Heading: Stock futures tick higher after Dow closes above 40,000 for the first time: Live updates\n",
      "Date: No date provided\n",
      "Link: https://www.cnbc.com/2024/05/19/stock-market-today-live-updates.html\n",
      "--------------------------------------------------\n",
      "Heading: Soaring debt and deficits causing worry about threats to the economy and markets\n",
      "Date: No date provided\n",
      "Link: https://www.cnbc.com/2024/05/19/soaring-debt-and-deficits-causing-worry-about-threats-to-the-economy-and-markets.html\n",
      "--------------------------------------------------\n",
      "Heading: Market sets record — Breaking down whether the bulls are in the clear or a peak is imminent\n",
      "Date: No date provided\n",
      "Link: https://www.cnbc.com/2024/05/18/market-sets-record-breaking-down-whether-the-bulls-are-in-the-clear-or-a-peak-is-imminent.html\n",
      "--------------------------------------------------\n",
      "Heading: Market sets record — Breaking down if the bulls are in the clear\n",
      "Date: No date provided\n",
      "Link: https://www.cnbc.com/2024/05/18/market-sets-record-breaking-down-whether-the-bulls-are-in-the-clear-or-a-peak-is-imminent.html\n",
      "--------------------------------------------------\n",
      "Heading: Ukraine, Russia exchange tit-for-tat strikes on border regions, leaving civilians dead and injured\n",
      "Date: No date provided\n",
      "Link: https://www.cnbc.com/2024/05/20/ukraine-war-live-updates-latest-news-on-russia-and-the-war-in-ukraine.html\n",
      "--------------------------------------------------\n",
      "Heading: Russian strikes on Ukraine's Kharkiv region kill at least 11\n",
      "Date: No date provided\n",
      "Link: https://www.cnbc.com/2024/05/19/russian-strikes-on-ukraines-kharkiv-region-kill-at-least-11.html\n",
      "--------------------------------------------------\n",
      "Heading: How much money the U.S. spends on war\n",
      "Date: No date provided\n",
      "Link: https://www.cnbc.com/2024/05/16/how-much-money-the-us-spends-on-war.html\n",
      "--------------------------------------------------\n",
      "Heading: How the U.S. spends trillions being the world's 'top cop'\n",
      "Date: No date provided\n",
      "Link: https://www.cnbc.com/video/2024/05/16/how-the-us-spends-trillions-being-the-worlds-top-cop.html\n",
      "--------------------------------------------------\n",
      "Heading: Putin heaps praise on Xi during visit; Russia says forces advancing 'in all directions' in Ukraine\n",
      "Date: No date provided\n",
      "Link: https://www.cnbc.com/2024/05/16/ukraine-war-live-updates-latest-news-on-russia-and-the-war-in-ukraine.html\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the CNBC World page\n",
    "url = 'https://www.cnbc.com/world/?region=world'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the sections containing the news articles\n",
    "articles = soup.find_all('div', class_='Card-titleContainer')\n",
    "\n",
    "# Initialize lists to store the extracted data\n",
    "headings = []\n",
    "dates = []\n",
    "links = []\n",
    "\n",
    "# Loop through each article section to extract data\n",
    "for article in articles[:10]:  # Limiting to the first 10 articles\n",
    "    # Extract the heading\n",
    "    heading = article.find('a').text.strip()\n",
    "    headings.append(heading)\n",
    "    \n",
    "    # Extract the link\n",
    "    link = article.find('a')['href']\n",
    "    links.append(link)\n",
    "    \n",
    "    # Extract the date\n",
    "    # Note: The date might be in a different tag or might not be present at all,\n",
    "    # you might need to adapt this depending on the actual HTML structure.\n",
    "    date = article.find('time')\n",
    "    if date:\n",
    "        date = date.text.strip()\n",
    "    else:\n",
    "        date = 'No date provided'\n",
    "    dates.append(date)\n",
    "\n",
    "# Print the extracted data\n",
    "for i in range(len(headings)):\n",
    "    print(f\"Heading: {headings[i]}\")\n",
    "    print(f\"Date: {dates[i]}\")\n",
    "    print(f\"Link: {links[i]}\")\n",
    "    print('-' * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f06c7b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Title, Date, Author(s)]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = \"https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloaded-articles/\"\n",
    "\n",
    "# Sending a request to fetch the content of the webpage\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Finding the relevant sections that contain the articles\n",
    "articles = soup.find_all('div', class_='article-item')\n",
    "\n",
    "# Lists to store the scraped data\n",
    "titles = []\n",
    "dates = []\n",
    "authors = []\n",
    "\n",
    "# Looping through each article and extracting the necessary information\n",
    "for article in articles:\n",
    "    title = article.find('h2', class_='article-title').text.strip()\n",
    "    date = article.find('div', class_='article-item__date').text.strip()\n",
    "    author = article.find('div', class_='article-item__authors').text.strip()\n",
    "    \n",
    "    titles.append(title)\n",
    "    dates.append(date)\n",
    "    authors.append(author)\n",
    "\n",
    "# Creating a DataFrame from the scraped data\n",
    "df = pd.DataFrame({\n",
    "    'Title': titles,\n",
    "    'Date': dates,\n",
    "    'Author(s)': authors\n",
    "})\n",
    "\n",
    "# Displaying the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Optionally, save the DataFrame to a CSV file\n",
    "df.to_csv('most_downloaded_articles.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7a13d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694837f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eebfb83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
